{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qTrunCDp6HAD"
      },
      "outputs": [],
      "source": [
        "#@title LICENSE\n",
        "# Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHA2XM4sTMfz"
      },
      "source": [
        "## JaxPruner Quick Start\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-research/jaxpruner/blob/main/colabs/quick_start.ipynb)\n",
        "\n",
        "This interactive colab provides a short overview of some of the key features of the `jaxpruner` library:\n",
        "\n",
        "- One-shot Pruning\n",
        "- Pruning during Optimization (Integration w/ optax)\n",
        "- ConfigDict Integration\n",
        "- Compatibility with JAX parallelization via `pmap` and `pjit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EdzHCJuop4ZB"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import flax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import PartitionSpec\n",
        "import jax.experimental.pjit\n",
        "import numpy as np\n",
        "import optax\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s2cBBZEk1_DG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/google-research/jaxpruner\n",
            "  Cloning https://github.com/google-research/jaxpruner to /private/var/folders/sq/g9fv1ssn3yqg_27cnkj1_c100000gr/T/pip-req-build-zhu9jesb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/jaxpruner /private/var/folders/sq/g9fv1ssn3yqg_27cnkj1_c100000gr/T/pip-req-build-zhu9jesb\n",
            "  Resolved https://github.com/google-research/jaxpruner to commit f133ee50f31a03d0152b8b272edb534065152f5d\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: chex in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jaxpruner==0.1) (0.1.88)\n",
            "Requirement already satisfied: flax in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jaxpruner==0.1) (0.10.2)\n",
            "Requirement already satisfied: jax in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jaxpruner==0.1) (0.4.38)\n",
            "Requirement already satisfied: jaxlib in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jaxpruner==0.1) (0.4.38)\n",
            "Requirement already satisfied: optax in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jaxpruner==0.1) (0.2.4)\n",
            "Requirement already satisfied: numpy in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jaxpruner==0.1) (2.2.1)\n",
            "Requirement already satisfied: ml-collections in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jaxpruner==0.1) (1.0.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from chex->jaxpruner==0.1) (2.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from chex->jaxpruner==0.1) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from chex->jaxpruner==0.1) (75.7.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from chex->jaxpruner==0.1) (1.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jax->jaxpruner==0.1) (0.5.0)\n",
            "Requirement already satisfied: opt_einsum in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jax->jaxpruner==0.1) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from jax->jaxpruner==0.1) (1.15.0)\n",
            "Requirement already satisfied: msgpack in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from flax->jaxpruner==0.1) (1.1.0)\n",
            "Requirement already satisfied: orbax-checkpoint in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from flax->jaxpruner==0.1) (0.11.0)\n",
            "Requirement already satisfied: tensorstore in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from flax->jaxpruner==0.1) (0.1.71)\n",
            "Requirement already satisfied: rich>=11.1 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from flax->jaxpruner==0.1) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from flax->jaxpruner==0.1) (6.0.2)\n",
            "Requirement already satisfied: six in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from ml-collections->jaxpruner==0.1) (1.17.0)\n",
            "Requirement already satisfied: etils[epy] in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from optax->jaxpruner==0.1) (1.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from rich>=11.1->flax->jaxpruner==0.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from rich>=11.1->flax->jaxpruner==0.1) (2.19.1)\n",
            "Requirement already satisfied: nest_asyncio in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from orbax-checkpoint->flax->jaxpruner==0.1) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from orbax-checkpoint->flax->jaxpruner==0.1) (5.29.2)\n",
            "Requirement already satisfied: humanize in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from orbax-checkpoint->flax->jaxpruner==0.1) (4.11.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from orbax-checkpoint->flax->jaxpruner==0.1) (3.19.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->jaxpruner==0.1) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from etils[epy]->optax->jaxpruner==0.1) (2024.12.0)\n",
            "Requirement already satisfied: importlib_resources in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from etils[epy]->optax->jaxpruner==0.1) (6.5.2)\n",
            "Requirement already satisfied: zipp in /Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages (from etils[epy]->optax->jaxpruner==0.1) (3.21.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/betheridge/personal/oss/fixjaxpruner/.venv/lib/python3.12/site-packages/ml_collections/config_dict/config_dict.py:163: SyntaxWarning: invalid escape sequence '\\['\n",
            "  index_match = re.match(\"(.*)\\[([0-9]+)\\]\", key)\n"
          ]
        }
      ],
      "source": [
        "# !pip3 install git+https://github.com/google-research/jaxpruner\n",
        "import jaxpruner\n",
        "import ml_collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaU4vsg-tfrq"
      },
      "source": [
        "\n",
        "# One-shot Pruning\n",
        "Pruning a given matrix to a desired level of sparsity is the building block of any pruning algorithm. Therefore jaxpruner provides a common API for one-shot\n",
        "pruning. This is achieved by calling the `instant_sparsify` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D-3sagbVuOCW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.28248537 0.0030005  0.5755601  0.80048716 0.24104941]\n",
            " [0.4520849  0.30063164 0.11872995 0.9262264  0.02943528]\n",
            " [0.77339077 0.9098681  0.30932128 0.9311595  0.7131189 ]\n",
            " [0.76767194 0.60364604 0.54187894 0.54719424 0.68919384]\n",
            " [0.6382148  0.9619373  0.9574717  0.21418309 0.21543407]]\n"
          ]
        }
      ],
      "source": [
        "matrix_size = 5\n",
        "learning_rate = 0.01\n",
        "matrix = jax.random.uniform(jax.random.PRNGKey(8), shape=(matrix_size, matrix_size))\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G51lY-u3xxa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.        0.        0.        0.        0.       ]\n",
            " [0.        0.        0.        0.9262264 0.       ]\n",
            " [0.        0.9098681 0.        0.9311595 0.       ]\n",
            " [0.        0.        0.        0.        0.       ]\n",
            " [0.        0.9619373 0.9574717 0.        0.       ]]\n",
            "uint8\n",
            "[[0 0 0 0 0]\n",
            " [0 0 0 1 0]\n",
            " [0 1 0 1 0]\n",
            " [0 0 0 0 0]\n",
            " [0 1 1 0 0]]\n"
          ]
        }
      ],
      "source": [
        "sparsity_distribution = functools.partial(\n",
        "    jaxpruner.sparsity_distributions.uniform, sparsity=0.8)\n",
        "pruner = jaxpruner.MagnitudePruning(sparsity_distribution_fn=sparsity_distribution)\n",
        "pruned_matrix, mask = pruner.instant_sparsify(matrix)\n",
        "\n",
        "print(pruned_matrix)\n",
        "print(mask.dtype)\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMXmrqvACdvA"
      },
      "source": [
        "We can quickly change the sparsity structure using `sparsity_type` flag. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JCwUUA2SCkTy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.80048716 0.        ]\n",
            " [0.         0.         0.         0.9262264  0.        ]\n",
            " [0.         0.         0.         0.9311595  0.        ]\n",
            " [0.76767194 0.         0.         0.         0.        ]\n",
            " [0.         0.9619373  0.         0.         0.        ]]\n",
            "uint8\n",
            "[[0 0 0 1 0]\n",
            " [0 0 0 1 0]\n",
            " [0 0 0 1 0]\n",
            " [1 0 0 0 0]\n",
            " [0 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "pruner = jaxpruner.MagnitudePruning(sparsity_distribution_fn=sparsity_distribution,\n",
        "                                    sparsity_type=jaxpruner.sparsity_types.NByM(1, 5))\n",
        "pruned_matrix, mask = pruner.instant_sparsify(matrix)\n",
        "\n",
        "print(pruned_matrix)\n",
        "print(mask.dtype)\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W3fHEmMEpk6"
      },
      "source": [
        "`instant sparsify` also supports parameter collections, which are commonly used in deep learning. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w7swztVv1pwN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'inv': Array([[0.        , 0.9969995 , 0.        , 0.        , 0.        ],\n",
            "       [0.        , 0.        , 0.        , 0.        , 0.9705647 ],\n",
            "       [0.        , 0.        , 0.6906787 , 0.        , 0.        ],\n",
            "       [0.        , 0.        , 0.45812106, 0.        , 0.        ],\n",
            "       [0.        , 0.        , 0.        , 0.7858169 , 0.        ]],      dtype=float32),\n",
            " 'pos': Array([[0.        , 0.        , 0.        , 0.80048716, 0.        ],\n",
            "       [0.        , 0.        , 0.        , 0.9262264 , 0.        ],\n",
            "       [0.        , 0.        , 0.        , 0.9311595 , 0.        ],\n",
            "       [0.76767194, 0.        , 0.        , 0.        , 0.        ],\n",
            "       [0.        , 0.9619373 , 0.        , 0.        , 0.        ]],      dtype=float32)}\n"
          ]
        }
      ],
      "source": [
        "# params = [matrix, 1 - matrix]\n",
        "params = {'pos': matrix, 'inv': 1 - matrix}\n",
        "pruned_params, masks = pruner.instant_sparsify(params)\n",
        "pprint.pprint(pruned_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E53ARCZdE8a-"
      },
      "source": [
        "It is common to choose different sparsities for different layers or keep them dense entirely. We provide some basic functions to distribute sparsity across different layers such as `uniform` (default) and `erk` under `jaxpruner.sparsity_distributions`. Users can also define their own distributions easily. Here we define a custom distribution function to set different sparsities for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XN2954cfFChC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_nparams': Array(50., dtype=float32),\n",
            " '_nparams_nnz': Array(20., dtype=float32),\n",
            " '_total_sparsity': Array(0.6, dtype=float32),\n",
            " 'inv': Array(0.8, dtype=float32),\n",
            " 'pos': Array(0.39999998, dtype=float32)}\n"
          ]
        }
      ],
      "source": [
        "def custom_distribution(params, sparsity=0.8):\n",
        "  return {key: 0.4 if 'pos' in key else sparsity for key in params}\n",
        "\n",
        "pruner = jaxpruner.MagnitudePruning(sparsity_distribution_fn=custom_distribution)\n",
        "pruned_params, masks = pruner.instant_sparsify(params)\n",
        "pprint.pprint(jaxpruner.summarize_sparsity(pruned_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbL4NxLopEY1"
      },
      "source": [
        "Masks used for enforcing sparsity use the same tree structure as the parameters pruned. We use `None` values to indicate dense parameters. We don't create masks for dense variables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nOaXL58kpZfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'inv': Array([[0, 1, 0, 0, 0],\n",
            "       [0, 0, 1, 0, 1],\n",
            "       [0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 1]], dtype=uint8),\n",
            " 'pos': None}\n"
          ]
        }
      ],
      "source": [
        "def custom_distribution2(params, sparsity=0.8):\n",
        "  return {key: None if 'pos' in key else sparsity for key in params}\n",
        "\n",
        "pruner = jaxpruner.MagnitudePruning(sparsity_distribution_fn=custom_distribution2)\n",
        "_, masks = pruner.instant_sparsify(params)\n",
        "pprint.pprint(masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64jmu55UHzIf"
      },
      "source": [
        "Changing the pruning algorithm is easy as they all inherit from the same `BaseUpdater`. We have the following baseline pruning and sparse training algorithms included in our library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VZUIsC1NH5JV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no_prune <class 'jaxpruner.base_updater.NoPruning'>\n",
            "magnitude <class 'jaxpruner.algorithms.pruners.MagnitudePruning'>\n",
            "random <class 'jaxpruner.algorithms.pruners.RandomPruning'>\n",
            "saliency <class 'jaxpruner.algorithms.pruners.SaliencyPruning'>\n",
            "magnitude_ste <class 'jaxpruner.algorithms.ste.SteMagnitudePruning'>\n",
            "random_ste <class 'jaxpruner.algorithms.ste.SteRandomPruning'>\n",
            "global_magnitude <class 'jaxpruner.algorithms.global_pruners.GlobalMagnitudePruning'>\n",
            "global_saliency <class 'jaxpruner.algorithms.global_pruners.GlobalSaliencyPruning'>\n",
            "static_sparse <class 'jaxpruner.algorithms.sparse_trainers.StaticRandomSparse'>\n",
            "rigl <class 'jaxpruner.algorithms.sparse_trainers.RigL'>\n",
            "set <class 'jaxpruner.algorithms.sparse_trainers.SET'>\n"
          ]
        }
      ],
      "source": [
        "for k in jaxpruner.ALGORITHM_REGISTRY:\n",
        "  print(k, jaxpruner.ALGORITHM_REGISTRY[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abfYB4LgH6mB"
      },
      "source": [
        "Next we use gradient based saliency score for pruning. `SaliencyPruning` requires gradients to be passed to `pruner.instant_sparsify`. Gradients are multipled with parameter values to obtain a first order Taylor approximation of the change in loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n_1waBgM2Epe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.5755601  0.         0.        ]\n",
            " [0.4520849  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.60364604 0.54187894 0.54719424 0.        ]\n",
            " [0.         0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Gradient based pruning\n",
        "pruner = jaxpruner.SaliencyPruning(sparsity_distribution_fn=sparsity_distribution)\n",
        "print(pruner.instant_sparsify(matrix, grads=(1 - matrix))[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcdznCj41jat"
      },
      "source": [
        "# Pruning as optimization (jaxpruner + optax)\n",
        "\n",
        "Often state-of-the-art pruning algorithms require iterative adjustments to the sparsity masks used. Such iterative approaches are stateful, i.e. they require some additional variables like masks, counters and initial values. This is similar to common optimization algorithms such as Adam and SGD+Momentum which require moving averages.\n",
        "\n",
        "The observation that *most iterative pruning and sparse training algoritms can be implemented as an optimizer*, played a key role when designing `jaxpruner` and led us to integrate `jaxpruner` with the `optax` optimization library.\n",
        "\n",
        "Here is an example training loop where we find an orthogonal matrix using gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZPVLns38l-sS"
      },
      "outputs": [],
      "source": [
        "matrix_size = 5\n",
        "\n",
        "def loss_fn(params):\n",
        "  matrix = params['w']\n",
        "  loss = jnp.sum((matrix @ matrix.T - jnp.eye(matrix_size))**2)\n",
        "  return loss\n",
        "\n",
        "grad_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "@functools.partial(jax.jit, static_argnames='optimizer')\n",
        "def update_fn(params, opt_state, optimizer):\n",
        "  loss, grads = grad_fn(params)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss\n",
        "\n",
        "def run_experiment(init_matrix):\n",
        "  optimizer = optax.sgd(0.05)\n",
        "  params = {'w': init_matrix}\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  for i in range(20):\n",
        "    params, opt_state, loss = update_fn(params, opt_state, optimizer)\n",
        "    if i % 4 == 0:\n",
        "      print(f'Step: {i}, loss: {loss}')\n",
        "  return params['w']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcOEMg-eUROR"
      },
      "source": [
        "First run the baseline training with a dense matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "imi-PawzUS_6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0, loss: 50.740779876708984\n",
            "Step: 4, loss: 1.0821640491485596\n",
            "Step: 8, loss: 0.5029404163360596\n",
            "Step: 12, loss: 0.09175926446914673\n",
            "Step: 16, loss: 0.0036935126408934593\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array([[-0.5452558 , -0.68374544,  0.39526057,  0.27897137, -0.01504929],\n",
              "       [ 0.18433541, -0.26916775, -0.45707935,  0.31126106, -0.76552254],\n",
              "       [-0.6820641 ,  0.08165327, -0.69178694, -0.14950337,  0.15661548],\n",
              "       [ 0.33259526, -0.6524972 , -0.23232982, -0.6030979 ,  0.20593995],\n",
              "       [-0.2942041 ,  0.16224197,  0.3196011 , -0.66244096, -0.5882758 ]],      dtype=float32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = jax.random.uniform(jax.random.PRNGKey(8),\n",
        "                            shape=(matrix_size, matrix_size))\n",
        "run_experiment(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMrwGVcOS9Sz"
      },
      "source": [
        "Adding a pruner to an existing training loop requires just 2 lines. First we wrap an existing optimizer using the `pruner.wrap_optax` method. This wrapped optimizer ensures the masks are updated during the training. Second, we add a `pruner.post_gradient_update` call after our gradient step. This function defines algorithm specific parameter updates (like applying a mask to parameters) and provides flexibility when implementing various algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "plZ5bqDhT4cR"
      },
      "outputs": [],
      "source": [
        "def run_pruning_experiment(init_matrix, pruner):\n",
        "  optimizer = optax.sgd(0.05)\n",
        "  # Modification #1\n",
        "  optimizer = pruner.wrap_optax(optimizer)\n",
        "\n",
        "  params = {'w': init_matrix}\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  for i in range(20):\n",
        "    params, opt_state, loss = update_fn(params, opt_state, optimizer)\n",
        "    # Modification #2\n",
        "    params = pruner.post_gradient_update(params, opt_state)\n",
        "\n",
        "    if i % 4 == 0:\n",
        "      print(f'Step: {i}, loss: {loss}')\n",
        "      print(jaxpruner.summarize_sparsity(params, only_total_sparsity=True))\n",
        "  return params['w']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0IifenuM9Ez"
      },
      "source": [
        "Now, prune the matrix in one step (step=15).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TqaMojfRNGHd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0, loss: 50.740779876708984\n",
            "{'_total_sparsity': Array(0., dtype=float32), '_nparams_nnz': Array(25., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 4, loss: 1.0821640491485596\n",
            "{'_total_sparsity': Array(0., dtype=float32), '_nparams_nnz': Array(25., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 8, loss: 0.5029404163360596\n",
            "{'_total_sparsity': Array(0., dtype=float32), '_nparams_nnz': Array(25., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 12, loss: 1.8950453996658325\n",
            "{'_total_sparsity': Array(0.8, dtype=float32), '_nparams_nnz': Array(5., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 16, loss: 1.3515889644622803\n",
            "{'_total_sparsity': Array(0.8, dtype=float32), '_nparams_nnz': Array(5., dtype=float32), '_nparams': Array(25., dtype=float32)}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array([[-0.        , -0.99310285,  0.        ,  0.        , -0.        ],\n",
              "       [ 0.        , -0.        , -0.        ,  0.        , -0.9267426 ],\n",
              "       [-0.        ,  0.        , -0.9929704 , -0.        ,  0.        ],\n",
              "       [ 0.        , -0.        , -0.        , -0.        ,  0.        ],\n",
              "       [-0.        ,  0.        ,  0.        , -0.9202637 , -0.2308394 ]],      dtype=float32)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    scheduler=jaxpruner.sparsity_schedules.OneShotSchedule(target_step=10)\n",
        "    )\n",
        "params = jax.random.uniform(jax.random.PRNGKey(8),\n",
        "                            shape=(matrix_size, matrix_size))\n",
        "run_pruning_experiment(params, pruner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXphssCebrrx"
      },
      "source": [
        "Alternatively we can prune it iteratively using the [polynomial schedule](https://arxiv.org/abs/1710.01878)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Tg9MAEYsO8YU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0, loss: 50.740779876708984\n",
            "{'_total_sparsity': Array(0., dtype=float32), '_nparams_nnz': Array(25., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 4, loss: 1.0821640491485596\n",
            "{'_total_sparsity': Array(0.48000002, dtype=float32), '_nparams_nnz': Array(13., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 8, loss: 1.2138574123382568\n",
            "{'_total_sparsity': Array(0.76, dtype=float32), '_nparams_nnz': Array(6., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 12, loss: 1.3473515510559082\n",
            "{'_total_sparsity': Array(0.8, dtype=float32), '_nparams_nnz': Array(5., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 16, loss: 1.0177024602890015\n",
            "{'_total_sparsity': Array(0.8, dtype=float32), '_nparams_nnz': Array(5., dtype=float32), '_nparams': Array(25., dtype=float32)}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array([[-0.        , -0.9976892 ,  0.        ,  0.        , -0.        ],\n",
              "       [ 0.        , -0.        , -0.        ,  0.        , -0.9915333 ],\n",
              "       [-0.        , -0.        , -0.99801445, -0.        ,  0.        ],\n",
              "       [ 0.        , -0.        , -0.        , -0.67936283,  0.        ],\n",
              "       [-0.        ,  0.        ,  0.        , -0.7284256 , -0.        ]],      dtype=float32)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    scheduler=jaxpruner.sparsity_schedules.PolynomialSchedule(\n",
        "        update_freq=4, update_start_step=2, update_end_step=14)\n",
        ")\n",
        "params = jax.random.uniform(jax.random.PRNGKey(8),\n",
        "                            shape=(matrix_size, matrix_size))\n",
        "run_pruning_experiment(params, pruner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49mkcCCzTQDQ"
      },
      "source": [
        "# ml_collections.ConfigDict Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlb2Vp36b6F-"
      },
      "source": [
        "Many popular jax libraries like [scenic](https://github.com/google-research/scenic) and [big_vision](https://github.com/google-research/big_vision) use `ml_collections.ConfigDict` to configure experiments. `jaxpruner` provides a helper function (`jaxpruner.create_updater_from_config`) to make it easy to use a `ConfigDict` to generate pruner objects. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TKY_GSumHYRI"
      },
      "outputs": [],
      "source": [
        "sparsity_config = ml_collections.ConfigDict()\n",
        "sparsity_config.algorithm = 'magnitude'\n",
        "sparsity_config.update_freq = 2\n",
        "sparsity_config.update_end_step = 15\n",
        "sparsity_config.update_start_step = 5\n",
        "sparsity_config.sparsity = 0.6\n",
        "sparsity_config.dist_type = 'uniform'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Uqqg0sYIHWNm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0, loss: 50.740779876708984\n",
            "{'_total_sparsity': Array(0., dtype=float32), '_nparams_nnz': Array(25., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 4, loss: 1.0821640491485596\n",
            "{'_total_sparsity': Array(0., dtype=float32), '_nparams_nnz': Array(25., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 8, loss: 0.7265740036964417\n",
            "{'_total_sparsity': Array(0.48000002, dtype=float32), '_nparams_nnz': Array(13., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 12, loss: 1.3116353750228882\n",
            "{'_total_sparsity': Array(0.6, dtype=float32), '_nparams_nnz': Array(10., dtype=float32), '_nparams': Array(25., dtype=float32)}\n",
            "Step: 16, loss: 1.129997730255127\n",
            "{'_total_sparsity': Array(0.6, dtype=float32), '_nparams_nnz': Array(10., dtype=float32), '_nparams': Array(25., dtype=float32)}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Array([[-0.5102305 , -0.6804417 ,  0.        ,  0.        , -0.        ],\n",
              "       [ 0.        , -0.        , -0.        ,  0.5152243 , -0.8074678 ],\n",
              "       [-0.44380936,  0.        , -0.8579842 , -0.        ,  0.        ],\n",
              "       [ 0.        , -0.55724764, -0.        , -0.38684854,  0.        ],\n",
              "       [-0.        ,  0.        ,  0.        , -0.72127473, -0.58689326]],      dtype=float32)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a dense layer and sparsify.\n",
        "pruner = jaxpruner.create_updater_from_config(sparsity_config)\n",
        "params = jax.random.uniform(jax.random.PRNGKey(8),\n",
        "                            shape=(matrix_size, matrix_size))\n",
        "run_pruning_experiment(params, pruner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16wgrFxCcGXp"
      },
      "source": [
        "# Parallelization with `pmap` and `pjit`\n",
        "\n",
        "The `jaxpruner` library is in general compatible with JAX parallelization mechanisms like `pmap` and `pjit`. There are some minor points to watch out for,\n",
        "which we will now demonstrate using parallelized versions of the previously introduced orthogonal matrix optimization example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDE-QFfCcHSe"
      },
      "source": [
        "## `pmap`\n",
        "\n",
        "First, we demonstrate compatibility with `pmap` where a model is replicated to run different shards of a batch on different devices. Note that this example\n",
        "has no actual model \"inputs\" apart from the parameter matrix and the replication is thus not directly useful, but the general mechanisms are the same as for real training.\n",
        "\n",
        "The main point to watch out for is to make sure that the optimizer state is replicated **after** wrapping it with the `jaxpruner`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T8EchfPLEei-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0, loss: 343.5727844238281\n",
            "Step: 5, loss: 70.89682006835938\n",
            "Step: 10, loss: 47.94889831542969\n",
            "Step: 15, loss: 34.66495132446289\n",
            "Step: 20, loss: 26.278722763061523\n",
            "Step: 25, loss: 20.651508331298828\n",
            "Step: 30, loss: 16.700218200683594\n",
            "Step: 35, loss: 13.826349258422852\n",
            "Step: 40, loss: 11.676509857177734\n",
            "Step: 45, loss: 10.030875205993652\n",
            "Step: 50, loss: 8.746789932250977\n",
            "Step: 55, loss: 7.728399276733398\n",
            "Step: 60, loss: 6.909374713897705\n",
            "Step: 65, loss: 6.242647647857666\n",
            "Step: 70, loss: 5.694087505340576\n",
            "Step: 75, loss: 5.238475799560547\n",
            "Step: 80, loss: 4.856863975524902\n",
            "Step: 85, loss: 4.534801483154297\n",
            "Step: 90, loss: 4.261125087738037\n",
            "Step: 95, loss: 4.027107238769531\n",
            "[[ 0.73600227 -0.         -0.         -0.         -0.         -0.\n",
            "  -0.         -0.8174586 ]\n",
            " [-0.          0.88034064 -0.         -0.          0.         -0.\n",
            "  -0.         -0.4709545 ]\n",
            " [-0.7798368  -0.         -0.         -0.          0.          0.\n",
            "  -0.         -0.7573714 ]\n",
            " [ 0.         -0.         -1.0685847   0.         -0.         -0.\n",
            "   0.          0.        ]\n",
            " [ 0.          0.          0.          1.17047    -0.          0.\n",
            "   0.          0.        ]\n",
            " [-0.53588206 -0.7215144   0.          0.          0.         -0.\n",
            "  -0.         -0.        ]\n",
            " [ 0.         -0.         -0.4267249   0.          0.7517653   0.7708804\n",
            "   0.          0.        ]\n",
            " [ 0.         -0.          0.         -0.          0.         -0.\n",
            "   0.          0.        ]]\n"
          ]
        }
      ],
      "source": [
        "matrix_size = 8\n",
        "\n",
        "def loss_fn(params):\n",
        "  matrix = params['w']\n",
        "  loss = jnp.sum((matrix @ matrix.T - jnp.eye(matrix_size))**2)\n",
        "  return loss\n",
        "\n",
        "grad_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "@functools.partial(\n",
        "    jax.pmap, out_axes=(0, 0, None), axis_name='batch',\n",
        "    static_broadcasted_argnums=(2,)\n",
        ")\n",
        "def update_fn(params, opt_state, optimizer):\n",
        "  loss, grads = grad_fn(params)\n",
        "  loss = jax.lax.pmean(loss, 'batch')\n",
        "  grads = jax.lax.pmean(grads, 'batch')\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss\n",
        "\n",
        "\n",
        "sparsity_distribution = functools.partial(\n",
        "    jaxpruner.sparsity_distributions.uniform, sparsity=0.8)\n",
        "\n",
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    scheduler=jaxpruner.sparsity_schedules.OneShotSchedule(target_step=0)\n",
        ")\n",
        "\n",
        "optimizer = optax.sgd(0.001)\n",
        "optimizer = pruner.wrap_optax(optimizer)\n",
        "params = {\n",
        "    'w': jax.random.normal(jax.random.PRNGKey(0), (matrix_size, matrix_size))\n",
        "}\n",
        "opt_state = optimizer.init(params)\n",
        "# The key step for using pmap with the jaxpruner is to replicate the optimizer\n",
        "# state **after** wrapping it.\n",
        "opt_state = flax.jax_utils.replicate(opt_state)\n",
        "params = flax.jax_utils.replicate(params)\n",
        "\n",
        "for i in range(100):\n",
        "  params, opt_state, loss = update_fn(params, opt_state, optimizer)\n",
        "  params = pruner.post_gradient_update(params, opt_state)\n",
        "  if i % 5 == 0:\n",
        "    print(f'Step: {i}, loss: {loss}')\n",
        "params = flax.jax_utils.unreplicate(params)\n",
        "print(params['w'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UObDjGBPcTYd"
      },
      "source": [
        "## `pjit`\n",
        "\n",
        "Next, we demonstrate tensor sharded training with `pjit`. Here the key is that the partition specifications of the wrapped optimizer state have to incoporate also the `jaxpruner.base_update.SparseState` produced by the pruning wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Vp0KIflicNNa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0, loss: 343.5727844238281\n",
            "Step: 5, loss: 70.89682006835938\n",
            "Step: 10, loss: 47.94889831542969\n",
            "Step: 15, loss: 34.66495132446289\n",
            "Step: 20, loss: 26.278722763061523\n",
            "Step: 25, loss: 20.651508331298828\n",
            "Step: 30, loss: 16.700218200683594\n",
            "Step: 35, loss: 13.826349258422852\n",
            "Step: 40, loss: 11.676509857177734\n",
            "Step: 45, loss: 10.030875205993652\n",
            "Step: 50, loss: 8.746789932250977\n",
            "Step: 55, loss: 7.728399276733398\n",
            "Step: 60, loss: 6.909374713897705\n",
            "Step: 65, loss: 6.242647647857666\n",
            "Step: 70, loss: 5.694087505340576\n",
            "Step: 75, loss: 5.238475799560547\n",
            "Step: 80, loss: 4.856863975524902\n",
            "Step: 85, loss: 4.534801483154297\n",
            "Step: 90, loss: 4.261125087738037\n",
            "Step: 95, loss: 4.027107238769531\n",
            "[[ 0.73600227 -0.         -0.         -0.         -0.         -0.\n",
            "  -0.         -0.8174586 ]\n",
            " [-0.          0.88034064 -0.         -0.          0.         -0.\n",
            "  -0.         -0.4709545 ]\n",
            " [-0.7798368  -0.         -0.         -0.          0.          0.\n",
            "  -0.         -0.7573714 ]\n",
            " [ 0.         -0.         -1.0685847   0.         -0.         -0.\n",
            "   0.          0.        ]\n",
            " [ 0.          0.          0.          1.17047    -0.          0.\n",
            "   0.          0.        ]\n",
            " [-0.53588206 -0.7215144   0.          0.          0.         -0.\n",
            "  -0.         -0.        ]\n",
            " [ 0.         -0.         -0.4267249   0.          0.7517653   0.7708804\n",
            "   0.          0.        ]\n",
            " [ 0.         -0.          0.         -0.          0.         -0.\n",
            "   0.          0.        ]]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────────────────────┐\n",
              "│                       │\n",
              "│                       │\n",
              "│                       │\n",
              "│                       │\n",
              "│         CPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>         │\n",
              "│                       │\n",
              "│                       │\n",
              "│                       │\n",
              "│                       │\n",
              "└───────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┌───────────────────────┐\n",
              "│                       │\n",
              "│                       │\n",
              "│                       │\n",
              "│                       │\n",
              "│         CPU \u001b[1;36m0\u001b[0m         │\n",
              "│                       │\n",
              "│                       │\n",
              "│                       │\n",
              "│                       │\n",
              "└───────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "matrix_size = 8\n",
        "if jax.device_count() % 8 == 0:\n",
        "  MESH_SHAPE = (2, 4)\n",
        "else:\n",
        "  MESH_SHAPE = (1, 1)\n",
        "\n",
        "def loss_fn(params):\n",
        "  matrix = params['w']\n",
        "  loss = jnp.sum((matrix @ matrix.T - jnp.eye(matrix_size))**2)\n",
        "  return loss\n",
        "\n",
        "grad_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "# Define the partition-specs for pjit; in most libraries for real models this\n",
        "# is done somewhat automatically, yet this will likely require a small\n",
        "# adjustment as shown below.\n",
        "\n",
        "params_partition = {\n",
        "    'w': PartitionSpec('X', 'Y')\n",
        "}\n",
        "\n",
        "# The main step required to run the jaxpruner together with pjit is defining\n",
        "# a partition-spec for the wrapped `SparseState` as shown below.\n",
        "opt_partition = jaxpruner.base_updater.SparseState(\n",
        "    masks=params_partition,\n",
        "    inner_state=(None, None),  # other optimizers may require sharding\n",
        "    target_sparsities=None,\n",
        "    count=None\n",
        ")\n",
        "\n",
        "resources = (params_partition, opt_partition)\n",
        "\n",
        "@functools.partial(\n",
        "    jax.experimental.pjit.pjit,\n",
        "    in_shardings=resources,\n",
        "    out_shardings=resources + (None,),\n",
        "    static_argnames='optimizer'\n",
        ")\n",
        "def update_fn(params, opt_state, optimizer):\n",
        "  loss, grads = grad_fn(params)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss\n",
        "\n",
        "\n",
        "sparsity_distribution = functools.partial(\n",
        "    jaxpruner.sparsity_distributions.uniform, sparsity=0.8)\n",
        "pruner = jaxpruner.MagnitudePruning(\n",
        "    sparsity_distribution_fn=sparsity_distribution,\n",
        "    scheduler=jaxpruner.sparsity_schedules.OneShotSchedule(target_step=0)\n",
        ")\n",
        "\n",
        "optimizer = optax.sgd(0.001)\n",
        "optimizer = pruner.wrap_optax(optimizer)\n",
        "params = {\n",
        "    'w': jax.random.normal(jax.random.PRNGKey(0), (matrix_size, matrix_size))\n",
        "}\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "devices = np.asarray(jax.devices()).reshape(MESH_SHAPE)\n",
        "mesh = jax.sharding.Mesh(devices, ('X', 'Y'))\n",
        "\n",
        "with mesh:\n",
        "  for i in range(100):\n",
        "    params, opt_state, loss = update_fn(params, opt_state, optimizer)\n",
        "    params = pruner.post_gradient_update(params, opt_state)\n",
        "    if i % 5 == 0:\n",
        "      print(f'Step: {i}, loss: {loss}')\n",
        "  print(params['w'])\n",
        "  jax.debug.visualize_array_sharding(params['w'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "quick_start.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
